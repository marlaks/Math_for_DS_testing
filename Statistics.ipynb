{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33b62ca3-4a1d-4c6d-b822-b57340101244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f126e-5952-4af5-bfe9-f33695e4e3e7",
   "metadata": {},
   "source": [
    "**Statistics Types**\n",
    "- descriptive statistics: used to describe and summarize data\n",
    "- inferential statistics: tries to uncover attributes of a larger population, usually based on a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8ccb9-18f1-467b-a30f-ae19c04617f2",
   "metadata": {},
   "source": [
    "**Population**\n",
    "- particular group of interest that should be studied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4e51fe-5f89-44b8-a1e2-b178db5b261d",
   "metadata": {},
   "source": [
    "**Sample**\n",
    "- subset of population that is ideally random an unbiased\n",
    "- is used to infer attributes about the population\n",
    "- population is usually to big, so samples are used\n",
    "- hard to get the bias out of the data though\n",
    "- ie how do i get a random sample of all universities in US to represent all students? Without injecting any selection bias?\n",
    "- *confirmation bias*: Selecting only data that supports your belief\n",
    "- *self-selection bias*: Selecting samples that are already pre-selected\n",
    "- *Survival bias*: Selects only the survivors, while the lost ones are not included in the sample\n",
    "- Computers don't recognize bias in data, that has to be done by the practicioner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6a8cb-cb0b-4c28-933d-aa3d9f92bc9e",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c04a2f-19c9-495d-9aed-501ff7398bf3",
   "metadata": {},
   "source": [
    "**Mean**\n",
    "- average\n",
    "- sample mean: x-bar\n",
    "- population mean: mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94d67bef-c7f5-46b5-9869-72640684782e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.25"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = [0,1,3,4,2,7,8,9]\n",
    "\n",
    "mean = sum(sample) / len(sample)\n",
    "mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d5eb8-caca-4530-bbe2-fb2d9df8f3bf",
   "metadata": {},
   "source": [
    "**Weighted Mean**\n",
    "- weight each data point before summing\n",
    "- then divide by sum of weights, not the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2116f62-998e-45ae-99fa-fd0ffb1c0caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.63333333333333"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = [9,23,65,23,78]\n",
    "weights = [3,7,45,3,2]\n",
    "\n",
    "weighted_mean = sum(s * w for s,w in zip(sample, weights)) / sum(weights)\n",
    "weighted_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc5764b-e56a-4250-90c3-03627693ae5e",
   "metadata": {},
   "source": [
    "**Median**\n",
    "- middle most value in a set of ordered values\n",
    "- if n is even, you average the two center most values\n",
    "- median is helpful alternative to mean if the data is skewed by outliers\n",
    "- when median is very different from mean, you have skewed dataset with many outliers\n",
    "- mean is the 50% quantile (from 25%, 50%, 75%)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725e76d3-7428-4470-a6a4-0f71b2f8feec",
   "metadata": {},
   "source": [
    "**Mode**\n",
    "- most frequent set of values in data set\n",
    "- useful in repetitive data and you want to know which value is most frequent\n",
    "- otherwise not used often\n",
    "- bimodal: 2 values with highest frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f485cb8-422e-46c1-924f-a243b4de38ef",
   "metadata": {},
   "source": [
    "**Variance**\n",
    "- measure for how spread out the data is\n",
    "- squared sum of differences between value and mean for each data point\n",
    "- then take the average\n",
    "- intuition: Average distance of a datapoint to the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef76199-c242-4f6b-aa05-264c451dc25e",
   "metadata": {},
   "source": [
    "**Standard Deviation**\n",
    "- square root of Variance. gets it closer to original scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda07715-9dd0-42ac-aa3b-8beb26cd005d",
   "metadata": {},
   "source": [
    "**Standard Deviation and Variance in Samples**\n",
    "- in samples, when dividing by the sample size, we divide by sample size - 1\n",
    "- we do this to decrease the bias\n",
    "- this increases the estimate of how spread out the data of the population is\n",
    "- since it is in any case most likely more spread out than the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d47e9-703b-4f04-9bbe-901412d58c5d",
   "metadata": {},
   "source": [
    "**The Normal Distribution**\n",
    "- most important distribution\n",
    "- most mass around its mean\n",
    "- is seen a lot in nature and sciences\n",
    "- *I think in all datasets where we can use normal distribution, this gives us a set of methods to infer certain characteristics about the dataset*\n",
    "- *properties*:\n",
    "- symmetrical: Both sides are identically mirrored at the mean\n",
    "- most mass is at center around mean\n",
    "- is has a spread (being narrow or wide) that is specified by standard deviation\n",
    "- tails are the least likely outcomes and approach, but never touch, zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d826ecd0-372b-4298-afe6-8465332e995d",
   "metadata": {},
   "source": [
    "**The probability Density Function (PDF)**\n",
    "- is the function that creates the normal distribution\n",
    "- can be used to look up likelyhoods at given values\n",
    "- its continuous, so we need to integrate it to get an area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e8d95-5f50-484d-ada9-d1c058d3acf6",
   "metadata": {},
   "source": [
    "**The cumulative distribution function**\n",
    "- finds the area under PDF (normal distribution)\n",
    "- like with beta distribution\n",
    "- ie whats the probability of a value being below x?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4893bde-7ed2-455d-9845-abbf8d9f10a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5755943933826899\n"
     ]
    }
   ],
   "source": [
    "mean = 64.43\n",
    "std_dev = 2.99\n",
    "x = 65\n",
    "\n",
    "P = norm.cdf(x, mean, std_dev)\n",
    "\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "079ba6da-7ad5-4a8e-8a48-c06065ac4d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4086326197841312"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of value being in certain area y - x\n",
    "\n",
    "mean = 70\n",
    "std_dev = 6.34\n",
    "x = 65\n",
    "y = 72\n",
    "\n",
    "P = norm.cdf(y, mean, std_dev) - norm.cdf(x, mean, std_dev)\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4132503b-c42d-4062-849d-e3515cb64bd9",
   "metadata": {},
   "source": [
    "**Inverse CDF**\n",
    "- opposite of CDF\n",
    "- under what value do x% of data points fall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ad0d2dc-2906-4e7e-b4ff-fd0e61b89b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.3481123445849"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# under what value do 95% of data points fall?\n",
    "# loc = mean, scale = std_dev\n",
    "\n",
    "x = norm.ppf(.95, loc = 64.43, scale = 2.99)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017ff8c-1ceb-422b-af45-178bd48e645c",
   "metadata": {},
   "source": [
    "**Z scores**\n",
    "- standard normal distribution: mean = 0, std_dev = 1)\n",
    "- normal distributions are often scaled to standard normal distributions to make them more comparable\n",
    "- standard normal distribution expresses all x values in terms of std_dev.\n",
    "- Amount of std_devs is called the Z-Score\n",
    "- Z = (x - mean)/std_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c923e197-3ae5-4272-9f1d-c200e2c97522",
   "metadata": {},
   "source": [
    "**Coefficient of variation**\n",
    "- metric to discribe the spread of a distribution\n",
    "- cv = std_dev/mean\n",
    "- basically adjusts the std_dev for the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2204fd4-d934-47d8-815d-4e856708d04b",
   "metadata": {},
   "source": [
    "## Inferential Statistics\n",
    "- Infer stuff about population by using sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150d87e3-9951-4d66-9b2a-2e210dec5590",
   "metadata": {},
   "source": [
    "**Central Limit Theorem**\n",
    "- uniform distribution: any value is equally likely, distribution becomes flat\n",
    "- but when these values are grouped and averaged, they become a normal distribution\n",
    "- *central limit theorem*: Interesting things happen when we take large enough samples of populations, calculate the mean of each sample, and plot as distribution:\n",
    "1. the mean of the sample means becomes the population mean\n",
    "2. if the population is normal, then the sample means will be normal\n",
    "3. if population is not normal, but sample size is greater 30, the sample means are still roughly normal distribution\n",
    "4. sample std_dev = population std_dev/sqrt(population size)\n",
    "- you need at least 31 samples to satisfy central limit theorem and see normal distribution\n",
    "- *then you can infer useful things about population using normal distribution, even if population is not normal*\n",
    "- less than 31 samples, you need to use T-distribution\n",
    "\n",
    "**probability distribution and sample size**\n",
    "- if population is very asymmetric, ie skewed, you need a lot bigger sample size to get the sample means looking like normal distribution\n",
    "- unimodal data: One peak. is good for central limit theorem\n",
    "- multimodal data: more peaks. need bigger sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266aae37-30ef-4e7a-9fc8-dd3ff8030bb9",
   "metadata": {},
   "source": [
    "**Confidence Intervals**\n",
    "\n",
    "-if we can get a sample of at least 31 data points, we can apply central limit theorem to calculate confidence interval, in which we have a certain confidence that a certain value will fall\n",
    "- Confidence Interval: Is the range in which a value falls\n",
    "- LOC (Level of confidence): The probability that a value falls in confidence interval\n",
    "- gives a confidence (ie 95% confidence (LOC)) that a specific value of the population (ie population mean) lies in a confidence Interval (ie 10 and 12) of the corresponding value of the sample\n",
    "- first choose a LOC (level of confidence) ie 95%\n",
    "- this is the desired confidence a value (ie population mean) should have\n",
    "- we can use central limit theorem to infer what this range for the population mean is\n",
    "\n",
    "*Calculating Confidence Interval*\n",
    "- First I need *critical z value*\n",
    "- critical z value leaves 95% probability in the center, meaning remaining 5% is split up in 2.5% on either side\n",
    "- *How is this symmetrical range containing 95% calculated?*\n",
    "- we can use inverse CDF to get the x values for .025 and .975, the space between represents the symmetrical 95% area around the center\n",
    "- then we use those x values and calculate the corresponding z values, that are the upper and lower bounds for the 95% center area\n",
    "- since we use standard normal distribution that is centered around mean 0, the upper and lower z values will be the same but other sign. so +z and - z\n",
    "- then calculate *margin of error*, which is the range around the sample mean that contains the population mean that level of confidence (95% certainty)\n",
    "- margin of error around the confidence interval = confidence interval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a47c6a-7428-43cf-bc6e-9d793b7c9c19",
   "metadata": {},
   "source": [
    "**Confidence Intervals my explanation**\n",
    "\n",
    "-You need: LOC, standard deviation, sample size\n",
    "- Confidence Interval: Interval in which I expect a value of the population to lie, based on the corresponding value of the sample\n",
    "- LOC: Level of confidence I want to have that the population value falls in Confidence Interval. Often 95%\n",
    "- to calculate the confidence interval, I have to calculate its upper and lower value, which hold 95% probability that the population value falls into this interval\n",
    "- We use inverse CDF to calculate two x values: X1 below which the bottom 2,5% of probabillity lies and X2 above which the top 2,5% of probability lie\n",
    "- this means the middle hold the 95% probability\n",
    "- Using those x values we calculate the corresponding z values (remember: standard normal distribuiton)\n",
    "- we can than use margin of erro formula to calculat a +- value\n",
    "- we can use this on the sample value to calculate the range (confidence interval) where population value is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "106ff90b-ecf5-4f6a-b190-53507cb3209a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63.68635915701992, 65.12964084298008)\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "def critical_z_value(p):\n",
    "    norm_dist = norm(loc=0.0, scale=1.0)\n",
    "    left_tail_area = (1.0 - p) / 2.0\n",
    "    upper_area = 1.0 - ((1.0 - p) / 2.0)\n",
    "    return norm_dist.ppf(left_tail_area), norm_dist.ppf(upper_area)\n",
    "\n",
    "\n",
    "def confidence_interval(p, sample_mean, sample_std, n):\n",
    "    # Sample size must be greater than 30\n",
    "\n",
    "    lower, upper = critical_z_value(p)\n",
    "    lower_ci = lower * (sample_std / sqrt(n))\n",
    "    upper_ci = upper * (sample_std / sqrt(n))\n",
    "\n",
    "    return sample_mean + lower_ci, sample_mean + upper_ci\n",
    "\n",
    "print(confidence_interval(p=.95, sample_mean=64.408, sample_std=2.05, n=31))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a36cb8-185c-4d95-8ba6-a5ea06bc11a8",
   "metadata": {},
   "source": [
    "**P values**\n",
    "- p value = probability that an experiment result occured by chance and not by the assumed explanation\n",
    "- Null Hypothesis (H0): Variable in question had no impact on the result and any positive result is random luck\n",
    "- Alternative Hypothesis (H1): The variable in question (control variable) is causing the positive result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67648822-e8d5-466c-b5fe-966984b1a401",
   "metadata": {},
   "source": [
    "**One Tailed Test**\n",
    "- H0: x >= certain value\n",
    "- H1: x < certain value\n",
    "- One Tailed because hypotheses is only tested on one side of a value (in this case < certain value)\n",
    "- significance level (alpha): if p value higher than this, reject sample\n",
    "- to calculate alpha use inverse cdf as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4396e440-ce56-46d8-99b1-2185f12486cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.53271955957279\n"
     ]
    }
   ],
   "source": [
    "# cold has 18 days mean recovery, 1.5 std_dev\n",
    "mean = 18\n",
    "std_dev = 1.5\n",
    "\n",
    "# what x value has 5% of probability under it?\n",
    "# using inverse cdf\n",
    "alpha = norm.ppf(0.05, mean, std_dev)\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1430867d-8540-4389-a47e-0b6dccd7b53c",
   "metadata": {},
   "source": [
    "- example above means:\n",
    "- if another sample has a mean of below 15.53, then the likelyhood that it is due to chance is below 5%\n",
    "- so if a cold treatment yields a mean recovery time of below 15.53, the likelyhood of this being due to chance and not the medicine is below 5%\n",
    "- so the medicine is statistically working\n",
    "- below: calculate p-value of sample that yielded mean of 16 to see if it is higher than alpha calculated above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78785e37-cef4-4f4a-864d-110e4e33e4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09121121972586788\n"
     ]
    }
   ],
   "source": [
    "# example where a drug yields a mean of 16\n",
    "# calculating p value: how likely is this result due to chance?\n",
    "\n",
    "# cold has 18 days mean recovery, 1.5 std_dev\n",
    "mean = 18\n",
    "std_dev = 1.5\n",
    "\n",
    "# p value of experiment that yields mean of 16\n",
    "p_value = norm.cdf(16, mean, std_dev)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678e9b5-baf7-4238-9389-82e1c1772b22",
   "metadata": {},
   "source": [
    "**Two tailed test**\n",
    "- looks on both sides of a value\n",
    "- using equal and not equal\n",
    "- H0: mean = 18\n",
    "- H1: mean != 18\n",
    "- now we are testing for statistical significance on each side\n",
    "- so if we set alpha to 5%, we split it to 2.5% on either side\n",
    "- if experiment mean is outside those areas, we reject H0\n",
    "- example below: if sample mean lands within those bounds, it is not statistically significant, because likelyhood of it being due to chance is hihgher than 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a647f2e7-393e-4822-9c3a-64daa76d0179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.060054023189918\n",
      "20.93994597681008\n"
     ]
    }
   ],
   "source": [
    "# calculating range for a 2 sided statistical significance of 5% (2.5% on each side)\n",
    "\n",
    "# cold has 18 days mean recovery and 1.5 std_dev\n",
    "mean = 18\n",
    "std_dev = 1.5\n",
    "\n",
    "# which x value has 2.5% of probability below it?\n",
    "x1 = norm.ppf(0.025, mean, std_dev)\n",
    "\n",
    "# which area has 97.5% of probability below it (so 2.5% probability above it)?\n",
    "x2 = norm.ppf(0.975, mean, std_dev)\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c723ed-6fba-4f31-b5f1-2989205d836b",
   "metadata": {},
   "source": [
    "- if calculating p-value for 2 tailed test, add the p-values for each side\n",
    "- example: population_mean = 18, sample_mean = 16\n",
    "- calculate p for below 16 and above 20\n",
    "- because we also take p for symmetrical other side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e3f92a5-97e1-4d4d-b96d-2bf2c70882d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18242243945173575\n"
     ]
    }
   ],
   "source": [
    "# cold has 18 days mean recovery, and std_dev 1.5\n",
    "mean = 18\n",
    "std_dev = 1.5\n",
    "\n",
    "# probability of 16 days or less\n",
    "p1 = norm.cdf(16, mean, std_dev)\n",
    "\n",
    "# probability of 20 days or more\n",
    "p2 = 1.0 - norm.cdf(20, mean, std_dev)\n",
    "\n",
    "# total probability of x being outside of significance threshold (alpha)\n",
    "p = p2 + p1\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b9607-1d2a-4b4d-abd0-c5010a7d3d7e",
   "metadata": {},
   "source": [
    "- above example means the outcome has a likelyhood of 18% to be down to chance\n",
    "- so it is statistically insignificant\n",
    "- two tailed tests are usually preferable\n",
    "- they make it more difficult to reject H0\n",
    "- and are not biased to only testing one side"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a428b28-c4ea-473a-9516-803d58daf7f2",
   "metadata": {},
   "source": [
    "**T-Distribution**\n",
    "- when dealing with 30 samples or less\n",
    "- T-distrbution is like normal distribution but with fatter tails\n",
    "- so more probability in the tails compared to normal distribution\n",
    "- reflects more variance and uncertainty\n",
    "- after approaching 31 items in sample, T-distribution reflects normal distribution\n",
    "- for confidence intervals you calculate critical t value, analog to critical z value for normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "994f59dc-6ffc-473d-afcc-f53f0a68b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.063898561628021 2.063898561628021\n"
     ]
    }
   ],
   "source": [
    "# get critical value range for 95% confidence interval\n",
    "# with sample size = 25\n",
    "# means: I am 95% confident the population mean falls within this value range of sample mean (if mean is taken as metric)\n",
    "\n",
    "n = 25\n",
    "lower = t.ppf(.025, df = n-1)\n",
    "upper = t.ppf(.975, df = n-1)\n",
    "\n",
    "print(lower,upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf90514-33fd-450f-91ca-df6d3c235fc7",
   "metadata": {},
   "source": [
    "- df = degrees of freedom and should be sample size - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d003f0dd-7377-4a14-8154-36df4e5ad33a",
   "metadata": {},
   "source": [
    "**Beyond the Mean**\n",
    "- in the example abpve we used confidence intervals and hypothesis testing with the mean\n",
    "- we can also use other values like variance and std_dev and proportions, but then we need other distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223f48d-ec48-4866-a5bb-4c5fa574a0db",
   "metadata": {},
   "source": [
    "**Big Data Considerations**\n",
    "- law of truly large numbers: Rare events are likely to be found, we just dont know which ones\n",
    "- so we cant really draw conclusions of finding a rare event at random\n",
    "- can we find it again with new data? Did we speculate it exists and then find it? Is there a good reasonable explanation for it?\n",
    "- if no: the rare event probably is completely random and no info should be infered from it\n",
    "- to not fall for this trap: Use structured hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb21df08-1457-460e-a6e9-217708dc270b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
